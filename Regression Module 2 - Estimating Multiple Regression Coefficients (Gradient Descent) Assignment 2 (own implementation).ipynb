{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "path = '/home/antoni/PycharmProjects/JupyterProjects/Coursera: ML : Regression/KCData/kc_house_train_data.csv'\n",
    "path_test = '/home/antoni/PycharmProjects/JupyterProjects/Coursera: ML : Regression/KCData/kc_house_test_data.csv'\n",
    "df = pd.read_csv(path)\n",
    "df_test = pd.read_csv(path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directions\n",
    "---------- -----------------\n",
    "\n",
    "Next write a function that takes a data set, a list of features (e.g. [‘sqft_living’, ‘bedrooms’]), to be used as inputs, and a name of the output (e.g. ‘price’). This function should return a features_matrix (2D array) consisting of first a column of ones followed by columns containing the values of the input features in the data set in the same order as the input list. It should also return an output_array which is an array of the values of the output in the data set (e.g. ‘price’). e.g. if you’re using SFrames and numpy you can complete the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(df, features, output):\n",
    "    inputs = df[features]\n",
    "    features_matrix = inputs.values\n",
    "    a = np.ones((len(df.index),1))\n",
    "    features_matrix = np.concatenate([np.ones((len(df.index),1)) ,features_matrix], axis=1)\n",
    "    output_array = output.values\n",
    "    return(features_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'price')\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 3.],\n",
       "        [1., 3.],\n",
       "        [1., 2.],\n",
       "        ...,\n",
       "        [1., 3.],\n",
       "        [1., 3.],\n",
       "        [1., 2.]]),\n",
       " array([221900., 538000., 180000., ..., 360000., 400000., 325000.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_numpy_data(X, ['bedrooms'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directions\n",
    "------------- -------\n",
    "If the features matrix (including a column of 1s for the constant) is stored as a 2D array (or matrix) and the regression weights are stored as a 1D array then the predicted output is just the dot product between the features matrix and the weights (with the weights on the right). Write a function ‘predict_output’ which accepts a 2D array ‘feature_matrix’ and a 1D array ‘weights’ and returns a 1D array ‘predictions’. e.g. in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outcome(feature_matrix, weights):\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directions\n",
    "----------- --\n",
    "\n",
    "If we have a the values of a single input feature in an array ‘feature’ and the prediction ‘errors’ (predictions - output) then the derivative of the regression cost function with respect to the weight of ‘feature’ is just twice the dot product between ‘feature’ and ‘errors’. Write a function that accepts a ‘feature’ array and ‘error’ array and returns the ‘derivative’ (a single number). e.g. in python:\n",
    "----------- --\n",
    "\n",
    "We have to remember the result [gradient]RSS(w) = -2 * H[transpose] (y-Hw) = -2 * H[transpose] (error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    derivative = 2* np.dot(feature, error)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  4]\n",
      " [20 16]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array(((10,20), (4,16)))\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.16227766, 4.47213595],\n",
       "       [2.        , 4.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directions\n",
    "------ -----\n",
    "Now we will use our predict_output and feature_derivative to write a gradient descent function. Although we can compute the derivative for all the features simultaneously (the gradient) we will explicitly loop over the features individually for simplicity. Write a gradient descent function that does the following:\n",
    "\n",
    "Accepts a numpy feature_matrix 2D array, a 1D output array, an array of initial weights, a step size and a convergence tolerance.\n",
    "\n",
    "While not converged updates each feature weight by subtracting the step size times the derivative for that feature given the current weights\n",
    "\n",
    "At each step computes the magnitude/length of the gradient (square root of the sum of squared components)\n",
    "\n",
    "When the magnitude of the gradient is smaller than the input tolerance returns the final weight vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was a top down attempt. We need to start from smalle elements, test them and then add them together. The first element we need is getting the partial derivative for all the features. Below is the second unassisted attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so to deal with the gradient we needed the gradient magnitude, a formula for which was given by the course : gradient_magnitude = sqrt(gradient_sum_squares). So below is third attempt with this new information. Its still a vector so I guess toleranc is also a vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converge = False\n",
    "    weights = initial_weights\n",
    "    while converge == False:\n",
    "        predicted = np.dot(feature_matrix, list(weights))\n",
    "        error = output - predicted\n",
    "        gradient = np.ones(((feature_matrix.shape[1]-1),1))\n",
    "        for i in range(gradient.shape[0]):\n",
    "            for feature in feature_matrix.T:\n",
    "                partial = 2 * np.dot(feature.T, error)\n",
    "                gradient[i] = partial\n",
    "        gradient_magnitude = np.sqrt(gradient)\n",
    "        if (gradient_magnitude < tolerance).all():\n",
    "            converge = True\n",
    "            return weights\n",
    "        else:\n",
    "            weights += (step_size * gradient.T)[0]\n",
    "            print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n",
    "--- ---\n",
    "Below will be a little test ot this gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'price')\n",
    "y = df.price\n",
    "simple_features = ['sqft_living']\n",
    "output_col= ['price']\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_matrix = get_numpy_data(X, ['sqft_living'], y)[0]\n",
    "my_output = get_numpy_data(X, ['sqft_living'], y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we ended up running to the problem we predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converge = False\n",
    "    weights = initial_weights\n",
    "    while converge == False:\n",
    "        predicted = np.dot(feature_matrix, list(weights))\n",
    "        error = output - predicted\n",
    "        gradient = np.ones(((feature_matrix.shape[1]-1),1))\n",
    "        for i in range(gradient.shape[0]):\n",
    "            for feature in feature_matrix.T:\n",
    "                partial = 2 * np.dot(feature.T, error)\n",
    "                gradient[i] = partial\n",
    "        gradient_magnitude = np.sqrt(gradient)\n",
    "        if (gradient_magnitude < tolerance).all():\n",
    "            converge = True\n",
    "            return weights\n",
    "        else:\n",
    "            weights += (step_size * gradient.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'price')\n",
    "y = df.price\n",
    "simple_features = ['sqft_living']\n",
    "output_col= ['price']\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7\n",
    "my_feature_matrix = get_numpy_data(X, ['sqft_living'], y)[0]\n",
    "my_output = get_numpy_data(X, ['sqft_living'], y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[267720. 582430. 226230. ... 405470. 445400. 370980.] [221900. 538000. 180000. ... 360000. 400000. 325000.]\n"
     ]
    }
   ],
   "source": [
    "error = my_output - np.dot(my_feature_matrix, initial_weights)\n",
    "print(error, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in my_feature_matrix.T:\n",
    "    partial = 2 * np.dot(feature.T, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.ones(((my_feature_matrix.shape[1]-1),1))\n",
    "for i in range(gradient.shape[0]):\n",
    "    for feature in my_feature_matrix.T:\n",
    "        partial = 2 * np.dot(feature.T, error)\n",
    "        gradient[i] = partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50551526703218.0\n"
     ]
    }
   ],
   "source": [
    "print (partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.05515267e+13]]\n"
     ]
    }
   ],
   "source": [
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gradient = np.ones((my_feature_matrix.shape[1], 1))\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-46719.28777939,    281.71222061])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_gradient_descent(my_feature_matrix, my_output, initial_weights, step_size, 2.5e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "f = pd.DataFrame(X['sqft_living'])\n",
    "lr.fit(f, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-47116.07907289371"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([281.95883963])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
